{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_TRAICING_V2'] = 'True'\n",
    "os.environ['LANCHAIN_ENDPOINT'] = 'https://api.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'xxxx'\n",
    "os.environ['FIRECRAWL_API_KEY'] = 'xxxx'\n",
    "os.environ['TAVILY_API_KEY'] = 'xxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: listen tcp 127.0.0.1:11434: bind: address already in use\n"
     ]
    }
   ],
   "source": [
    "! ollama serve\n",
    "local_llm = 'llama3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreive Documents\n",
    "\n",
    "- Use FireCrawl to scrape the contents of the URLs\n",
    "- Filter out metadata as FireCrawl returns metadata as an Array by defualt\n",
    "- Create a vector database using GPT4AllEmbeddings and the filtered documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Add as many URLs as you want\n",
    "urls = [\n",
    "    \"https://research.google/blog/advancing-personal-health-and-wellness-insights-with-ai/\"\n",
    "]\n",
    "\n",
    "docs = [FireCrawlLoader(api_key='xxxx', url=url, mode=\"scrape\").load() for url in urls]\n",
    "\n",
    "# Split documents\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split dociments\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Filter out complex metadata and ensure proper document formatting\n",
    "filtered_docs = []\n",
    "for doc in doc_splits:\n",
    "    # Ensure the doc is an instance of Document and has a 'metadata' attribute\n",
    "    if isinstance(doc, Document) and hasattr(doc, \"metadata\"):\n",
    "        clean_metadata = {k: v for k, v in doc.metadata.items() if isinstance(v, (str, int, float, bool))}\n",
    "        filtered_docs.append(Document(page_content=doc.page_content, metadata=clean_metadata))\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=filtered_docs,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=GPT4AllEmbeddings(model_name=\"all-MiniLM-L6-v2.gguf2.f16.gguf\"),\n",
    ")    \n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grade Documents\n",
    "Llama3 has a specific prompt style we need to follow.\n",
    "[Llama3 Prompt Style doc](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3)\n",
    "\n",
    "This is our first point to determine if the document is relevent to the users question\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begining_of_text|><|start_header_id|>system<|end+header_id|> You are a grader assessing relevance\n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "    grade it as relevant. It doe not need to be a stringent test. The goal is to filter out errroneous retriecals. \\n\n",
    "    Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary scroe as a JSON with a single key 'score' and no premable or explaination.  \n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {docunment} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"docunment\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"how to use llm for healthcare\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"docunment\": doc_txt}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it seems that Large Language Models (LLMs) can be used in healthcare by analyzing a broader range of health information, including medical records, nutrition data, and user-provided journal entries. This can potentially offer deeper insights and more effective guidance for personal health management as LLMs continue to advance.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begining_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"docunment\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"how to use llm for healthcare\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webs Search via Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader\n",
    "# LIN\n",
    "Llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begining_of_text|><|start_header_id|>system<|end+header_id|> You are a grader assessing whether\n",
    "    an answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the\n",
    "    answer is useful to resolve a question. Provide the binary scroe as a JSON with a single key 'score'\n",
    "    and no premable or explaination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the answer:\n",
    "    \\n ------ \\n\n",
    "    {generation}\n",
    "    \\n ------ \\n\n",
    "    Here is the question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begining_of_text|><|start_header_id|>system<|end+header_id|> You are a grader assessing whether\n",
    "    an answer is grounded in / supporteed by a set of facts. Give a binary score 'yes' or 'no' to indicate whether the\n",
    "    answer is grounded in / supporteed by a set of facts. Provide the binary scroe as a JSON with a single key 'score'\n",
    "    and no premable or explaination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts:\n",
    "    \\n ------ \\n\n",
    "    {documents}\n",
    "    \\n ------ \\n\n",
    "    Here is the answer: {generation} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"docunment\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lang graph - Setup states & nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "### State\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: wether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The currecnt graph state\n",
    "    \n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"----RETRIEVE----\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def grade_document(state):\n",
    "    \"\"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevent, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The currecnt graph state\n",
    "\n",
    "    Returns:\n",
    "        state(dict): Filtered out irrelevent documents and update web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"----CHECK DOCUMENT RELEVANCE TO THE QUESTION----\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Search each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"docunment\": d.page_content})\n",
    "        grade = score[\"score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRAFE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we need to run a web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The currecnt graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"----GENERATE----\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG Generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The currecnt graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Append web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"----WEB SEARCH----\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "### Conditional edge (Routing)\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Decide whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The currecnt graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Binary decision for next node to call\n",
    "    \"\"\"\n",
    "    print(\"----ASSESS GRADED DOCUMENTS----\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "    \n",
    "### Conditional edge\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Decide whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The currecnt graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Binary decision for next node to call\n",
    "    \"\"\"\n",
    "    print(\"----CHECK HALLUCINATIONS----\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    # Check hallucinations\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMNETS---\")\n",
    "        # Checkk the question-answer pair\n",
    "        print(\"---CHECK GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION IS NOT GROUNDED IN QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATON IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "    \n",
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search) # web search\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve documents\n",
    "workflow.add_node(\"grade_documents\", grade_document) # grade documents\n",
    "workflow.add_node(\"generate\", generate) # generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\", \n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----RETRIEVE----\n",
      "'Finished running: retrieve:'\n",
      "----CHECK DOCUMENT RELEVANCE TO THE QUESTION----\n",
      "----ASSESS GRADED DOCUMENTS----\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: grade_documents:'\n",
      "----GENERATE----\n",
      "----CHECK HALLUCINATIONS----\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMNETS---\n",
      "---CHECK GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "'Finished running: generate:'\n",
      "Based on the provided context, it seems that Large Language Models (LLMs) can be used in healthcare by analyzing a broader range of health information, including medical records, nutrition data, and user-provided journal entries. This can potentially offer deeper insights and more effective guidance for personal health management as LLMs continue to advance.\n"
     ]
    }
   ],
   "source": [
    "# Compile \n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"how to use llm for healthcare\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "print(value['generation'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
